
# lecture 1: Introduction to Computer Science

Computer Science is basically the study of how to solve problems using computers.
we take information, process it, and produce output.
Now before processing the information, we need to store it.
and before storing it we need to represent it in a way that the computer can understand.

# Representation of Information

Now if we talk about numbers we use different number systems to represent them.
For example Unary that uses 1s and 0s to represent numbers.

Unary is a base 1 number system. It is the simplest number system and uses only one digit, which is 1. In unary, the number 1 is represented as "1", the number 2 as "11", the number 3 as "111", and so on. Unary is not commonly used in practice, but it is useful for understanding the concept of counting and representation of numbers.

# Binary number system

And then we have Binary which is a base 2 number system. It uses two digits, 0 and 1, to represent numbers. In binary, the number 1 is represented as "1", the number 2 as "10", the number 3 as "11", the number 4 as "100", and so on. Binary is widely used in computer science because computers operate using binary logic.

Now if we focus on BIbary digIT and see BIT we come accross a new term called bit.
A bit is the smallest unit of data in a computer and can have a value of either 0 or 1.

Now lets assume a bit is a switch that can be either on (1) or off (0).

SO (010) will be equal to 0 * 2 ^ 2  + 1 * 2 ^ 1 + 0 * 2 ^ 0 in binary.
And (110) will be equal to 6 in binary.

similar to what we are familiar with in decimal system.
As decimal has 10 digits (0-9), binary has 2 digits (0-1).
So in decimal we have 10 different possibilites for each digit, while in binary we have 2 different possibilities for each digit.

Means (100) will be equal to 1 * 10 ^ 2 + 0 * 10 ^ 1 + 0 * 10 ^ 0 = 100 in decimal.

Now if we only have 0 and 1 to represent data Alphabets and other characters, we need to use a system that can represent all the characters we need.
So we use ASCII and Unicode.
ASCII is a character encoding standard that uses 7 bits to represent 128 characters, including letters, digits, and special symbols. For example, the letter "A" is represented as 65 in decimal and 1000001 in binary.
